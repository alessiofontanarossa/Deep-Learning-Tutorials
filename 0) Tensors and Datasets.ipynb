{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b2a3576",
   "metadata": {},
   "source": [
    "##### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ae20afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                    ########### VISUALIZATION TOOLS ###########\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt                  # basic plotting\n",
    "import matplotlib.ticker as mtick                 # axis tick formatting\n",
    "from matplotlib import colormaps as cm            # colormaps\n",
    "import matplotlib.patches as mpatches             # patch shapes\n",
    "from matplotlib.patches import Patch              # patch base class\n",
    "\n",
    "import seaborn as sns                             # statistical data visualization\n",
    "import warnings                                   # warning control\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # ignore seaborn UserWarnings\n",
    "\n",
    "import plotly.express as px                       # interactive plots (express interface)\n",
    "import plotly.graph_objects as go                  # interactive plots (graph objects)\n",
    "\n",
    "import folium                                    # interactive maps\n",
    "from folium import plugins                        # folium plugins\n",
    "from folium import Choropleth                      # choropleth maps\n",
    "\n",
    "from IPython.display import display, IFrame, HTML  # display rich content in notebooks\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from tqdm.notebook import tqdm                    # progress bars for notebook loops\n",
    "\n",
    "import wordcloud                                 # word cloud visualization\n",
    "from wordcloud import WordCloud, STOPWORDS        # word cloud generator and stopwords\n",
    "\n",
    "                                    ########### VECTORS AND MATRICES ###########\n",
    "\n",
    "import numpy as np                               # numerical arrays and matrix operations\n",
    "import pandas as pd                              # data manipulation and analysis\n",
    "\n",
    "                                    ########### SCIPY ###########\n",
    "\n",
    "from scipy import stats                           # statistical functions\n",
    "\n",
    "                                    ########### SCIKIT-LEARN ###########\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,      # z-score normalization\n",
    "    normalize,           # row-wise normalization\n",
    "    PolynomialFeatures,  # polynomial regression features\n",
    "    OneHotEncoder,       # one-hot encoding categorical features\n",
    "    LabelEncoder,        # label encoding for target\n",
    "    label_binarize       # multi-label one-hot encoding\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,    # ordinary least squares regression\n",
    "    Ridge,               # ridge regression (L2 regularization)\n",
    "    Lasso,               # lasso regression (L1 regularization)\n",
    "    LogisticRegression   # logistic regression for classification\n",
    ")\n",
    "\n",
    "from sklearn.tree import (\n",
    "    DecisionTreeClassifier,  # classification trees\n",
    "    DecisionTreeRegressor,   # regression trees\n",
    "    plot_tree                # visualize decision trees\n",
    ")\n",
    "\n",
    "from sklearn.svm import (\n",
    "    LinearSVC,  # linear support vector classifier\n",
    "    SVC         # kernelized SVM classifier\n",
    ")\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier  # k-nearest neighbors\n",
    "\n",
    "from sklearn.cluster import KMeans               # k-means clustering\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,    # ensemble regression method\n",
    "    RandomForestClassifier    # ensemble classification method\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,      # MSE regression metric\n",
    "    r2_score,               # R-squared metric\n",
    "    log_loss,               # cross-entropy loss\n",
    "    mean_absolute_error,    # MAE metric\n",
    "    root_mean_squared_error,# RMSE metric\n",
    "    confusion_matrix,       # confusion matrix\n",
    "    ConfusionMatrixDisplay, # plot confusion matrix\n",
    "    accuracy_score,         # classification accuracy\n",
    "    roc_auc_score,          # ROC AUC metric\n",
    "    classification_report,  # detailed classification metrics report\n",
    "    silhouette_score, silhouette_samples, davies_bouldin_score,  # clustering quality metrics\n",
    "    explained_variance_score # explained variance regression metric\n",
    ")\n",
    "\n",
    "from sklearn.pipeline import Pipeline           # pipeline for chaining transforms and estimators\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,       # split data into train/test sets\n",
    "    cross_val_score,        # k-fold cross-validation scores\n",
    "    cross_val_predict,      # k-fold cross-validation predictions\n",
    "    StratifiedKFold,        # stratified k-fold cross-validation\n",
    "    GridSearchCV            # grid search hyperparameter tuning\n",
    ")\n",
    "\n",
    "from sklearn.multiclass import OneVsOneClassifier  # multiclass classification using binary classifiers\n",
    "\n",
    "from sklearn.utils.class_weight import compute_sample_weight  # compute sample weights for imbalanced data\n",
    "\n",
    "from sklearn.decomposition import PCA              # principal component analysis\n",
    "\n",
    "from sklearn.compose import ColumnTransformer       # apply different transformations to columns\n",
    "\n",
    "from sklearn.impute import SimpleImputer             # missing value imputation\n",
    "\n",
    "#from umap.umap_ import UMAP                          # UMAP dimensionality reduction (commented out)\n",
    "\n",
    "from sklearn.manifold import TSNE                     # t-SNE dimensionality reduction\n",
    "\n",
    "from sklearn.datasets import (\n",
    "    make_blobs,              # synthetic cluster dataset generator\n",
    "    make_classification,     # synthetic classification dataset generator\n",
    "    load_diabetes,           # diabetes dataset\n",
    "    load_breast_cancer,      # breast cancer dataset\n",
    "    load_iris,               # iris dataset\n",
    "    fetch_california_housing # california housing dataset\n",
    ")\n",
    "diabetes = load_diabetes()\n",
    "cancers = load_breast_cancer()\n",
    "iris = load_iris()\n",
    "cal_housing = fetch_california_housing()\n",
    "\n",
    "                                    ########### ENSEMBLE METHODS ###########\n",
    "\n",
    "from xgboost import XGBRegressor                   # extreme gradient boosting regressor\n",
    "\n",
    "                                    ########### PYTORCH ###########\n",
    "\n",
    "import torch                                        # core PyTorch\n",
    "from torch import nn                               # neural network modules\n",
    "from torch import optim                            # optimizers\n",
    "import torch.nn.functional as functional           # functional interface for NN ops\n",
    "from torch.utils.data import Dataset, DataLoader, random_split  # data utilities\n",
    "from torchvision.utils import make_grid             # visualize image grids\n",
    "import torchvision                                  # computer vision models and datasets\n",
    "import torchvision.transforms as transforms         # image transforms pipeline\n",
    "from torchvision import datasets                     # image datasets\n",
    "from torch.utils.data import Subset                  # subset of datasets\n",
    "import torchvision.models as models                  # pretrained models\n",
    "\n",
    "import albumentations as albuments                   # advanced data augmentation\n",
    "\n",
    "from torchsummary import summary                     # model summary\n",
    "\n",
    "import timm                                         # PyTorch Image Models (SOTA model zoo)\n",
    "\n",
    "                                    ########### IMAGES ###########\n",
    "\n",
    "import os                                           # filesystem operations\n",
    "from PIL import Image, ImageFile                     # image loading and processing\n",
    "\n",
    "from copy import copy                                # shallow copy\n",
    "import operator                                     # functional tools for operators\n",
    "import sys                                          # system-specific parameters and functions\n",
    "\n",
    "import cv2                                          # OpenCV for image processing\n",
    "\n",
    "                                    ########### SCRAPING ###########\n",
    "\n",
    "import urllib                                        # URL handling\n",
    "import requests                                     # HTTP requests\n",
    "import io                                           # streams and bytes IO\n",
    "from io import StringIO                             # string stream for IO\n",
    "\n",
    "from bs4 import BeautifulSoup                        # HTML/XML parsing\n",
    "\n",
    "import json                                         # JSON parsing\n",
    "import xml.etree.ElementTree as ET                   # XML parsing\n",
    "\n",
    "                                    ########### MISC ###########\n",
    "                                    \n",
    "import gymnasium                                    # reinforcement learning environments\n",
    "from gymnasium.wrappers import RecordVideo          # video recording wrapper for gymnasium envs\n",
    "\n",
    "import glob                                         # Unix style pathname pattern expansion\n",
    "import base64                                       # base64 encoding/decoding\n",
    "\n",
    "import random                                       # random number generators\n",
    "import time                                         # time-related functions\n",
    "import datetime                                     # date and time functions\n",
    "\n",
    "import re                                           # regular expressions\n",
    "import unicodedata                                  # Unicode character database\n",
    "\n",
    "import shutil                                       # high-level file operations\n",
    "from pathlib import Path                            # filesystem path manipulations\n",
    "\n",
    "from collections import namedtuple, deque          # advanced data structures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243dbf42",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81763cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotVec(vectors):\n",
    "    ax = plt.axes()\n",
    "    \n",
    "    # For loop to draw the vectors\n",
    "    for vec in vectors:\n",
    "        ax.arrow(0, 0, *vec[\"vector\"], head_width = 0.05, color = vec[\"color\"], head_length = 0.1)\n",
    "        plt.text(*(vec[\"vector\"] + 0.1), vec[\"name\"])\n",
    "    \n",
    "    plt.ylim(-2,2)\n",
    "    plt.xlim(-2,2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2c1f0e",
   "metadata": {},
   "source": [
    "## 1D tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb53acce",
   "metadata": {},
   "source": [
    "### Types, size, reshape, list - Pandas - Numpy - Torch, slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe41fc",
   "metadata": {},
   "source": [
    "Basic torch tensor types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8dd6e796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_integers = [0, 1, 2, 3, 4]\n",
    "list_floats = [0.0, 1.0, 2.0, 3.0, 4.0]\n",
    "\n",
    "ints_to_tensor = torch.tensor(list_integers)\n",
    "floats_to_tensor = torch.tensor(list_floats)\n",
    "floats_int_tensor = torch.tensor(list_floats, dtype = torch.int64) # from floats to integers\n",
    "int_floats_tensor = torch.tensor(list_floats, dtype = torch.float32) # from integers to floats\n",
    "# int_floats_tensor = torch.FloatTensor(list_integers) # equivalent to the previous line\n",
    "\n",
    "ints_to_tensor.dtype # torch.int64 --> type of the entries\n",
    "ints_to_tensor.type() # torch.LongTensor --> torch type\n",
    "type(ints_to_tensor) # torch.Tensor --> Python type\n",
    "\n",
    "floats_to_tensor.dtype # torch.float32\n",
    "floats_to_tensor.type() # torch.FloatTensor\n",
    "type(ints_to_tensor) # torch.Tensor --> Python type\n",
    "\n",
    "floats_int_tensor.dtype # torch.int64 \n",
    "floats_int_tensor.type() # torch.LongTensor\n",
    "type(floats_int_tensor) # torch.Tensor\n",
    "\n",
    "int_floats_tensor.dtype # torch.float32\n",
    "int_floats_tensor.type() # torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846b89d0",
   "metadata": {},
   "source": [
    "Size of a tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c76f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ints_to_tensor.size() # torch.Size([5])\n",
    "ints_to_tensor.shape # torch.Size([5])\n",
    "ints_to_tensor.ndimension() # 1 (1D tensor)\n",
    "ints_to_tensor.numel() # 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add2a0e",
   "metadata": {},
   "source": [
    "Reshaping a tensor. The `x.view(-1,1)` is equivalent to `x.view(x.numel(),1)`, so that it is done automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7f603dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ints_to_tensor.view(5,1) # size --> torch.Size([5, 1])\n",
    "\n",
    "ints_to_tensor.view(-1,1) # size --> torch.Size([5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c2b5e2",
   "metadata": {},
   "source": [
    "List <--> Torch tensors and numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca1e827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_floats = [0.0, 1.0, 2.0, 3.0, 4.0]\n",
    "floats_to_tensor = torch.tensor(list_floats)\n",
    "list_floats_again = floats_to_tensor.tolist()\n",
    "list_floats_again == list_floats # True\n",
    "\n",
    "floats_to_tensor[3].item() # Python number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b00b731",
   "metadata": {},
   "source": [
    "Torch tensor <--> Numpy(<--> Pandas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262f7efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_series = pd.Series([0.1, 2, 0.3, 10.1])\n",
    "numpy_array = np.array(pandas_series.values)\n",
    "torch_tensor1D = torch.from_numpy(numpy_array) # torch.Size([5])\n",
    "numpy_array_again = torch_tensor1D.numpy()\n",
    "numpy_array_again == numpy_array # array([ True,  True,  True,  True])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557d5d7",
   "metadata": {},
   "source": [
    "Change one value and slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bbd89d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_sample = torch.tensor([20, 1, 2, 3, 4])\n",
    "tensor_sample[0] = 100\n",
    "tensor_sample # tensor([100,   1,   2,   3,   4])\n",
    "\n",
    "tensor_sample[1:4] #tensor([1, 2, 3])\n",
    "\n",
    "tensor_sample[[1,3]] #tensor([1, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d764494",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36102b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0635, 0.1269, 0.1904, 0.2539, 0.3173, 0.3808, 0.4443, 0.5077,\n",
       "        0.5712, 0.6347, 0.6981, 0.7616, 0.8251, 0.8885, 0.9520, 1.0155, 1.0789,\n",
       "        1.1424, 1.2059, 1.2693, 1.3328, 1.3963, 1.4597, 1.5232, 1.5867, 1.6501,\n",
       "        1.7136, 1.7771, 1.8405, 1.9040, 1.9675, 2.0309, 2.0944, 2.1579, 2.2213,\n",
       "        2.2848, 2.3483, 2.4117, 2.4752, 2.5387, 2.6021, 2.6656, 2.7291, 2.7925,\n",
       "        2.8560, 2.9195, 2.9829, 3.0464, 3.1099, 3.1733, 3.2368, 3.3003, 3.3637,\n",
       "        3.4272, 3.4907, 3.5541, 3.6176, 3.6811, 3.7445, 3.8080, 3.8715, 3.9349,\n",
       "        3.9984, 4.0619, 4.1253, 4.1888, 4.2523, 4.3157, 4.3792, 4.4427, 4.5061,\n",
       "        4.5696, 4.6331, 4.6965, 4.7600, 4.8235, 4.8869, 4.9504, 5.0139, 5.0773,\n",
       "        5.1408, 5.2043, 5.2677, 5.3312, 5.3947, 5.4581, 5.5216, 5.5851, 5.6485,\n",
       "        5.7120, 5.7755, 5.8389, 5.9024, 5.9659, 6.0293, 6.0928, 6.1563, 6.2197,\n",
       "        6.2832])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_tensor = torch.tensor([1.0, -1.0, 1, -1])\n",
    "# Methods of tensor objects. Aggregate methods\n",
    "math_tensor.mean() \n",
    "math_tensor.std()\n",
    "math_tensor.max()\n",
    "math_tensor.min() # tensor(-1.)\n",
    "math_tensor.min().item() # -1.0\n",
    "# Functions of Torch. They enter on each entries:\n",
    "pi_tensor = torch.tensor([0, np.pi/2, np.pi])\n",
    "torch.sin(pi_tensor)\n",
    "torch.linspace(0, 2*np.pi, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1740f0",
   "metadata": {},
   "source": [
    "### Operations between tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce1b0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([83.0, 22.0])\n",
    "v = torch.tensor([16.0, 42.5])\n",
    "\n",
    "u+v\n",
    "u-v\n",
    "u+1\n",
    "2*u\n",
    "u * v # Element-wise Product/Hadamard Product\n",
    "u @ v == torch.dot(u,v) # DOT: this works only if u and v have the same type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24b897b",
   "metadata": {},
   "source": [
    "## 2D tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef25c819",
   "metadata": {},
   "source": [
    "### Size, reshape, Pandas - Numpy - Torch, slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3232573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of elements in twoD_tensor:  9\n"
     ]
    }
   ],
   "source": [
    "twoD_list = [[11, 12, 13], [21, 22, 23], [31, 32, 33]]\n",
    "twoD_tensor = torch.tensor(twoD_list, dtype = torch.float32)\n",
    "\n",
    "twoD_tensor.ndimension() # 2 --> 2D tensor\n",
    "twoD_tensor.size() == twoD_tensor.shape # torch.Size([3, 3])\n",
    "\n",
    "twoD_tensor.numel() # 9 = 3x3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d232816",
   "metadata": {},
   "source": [
    "Pandas <--> Numpy <--> Torch tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7afe4700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_tensor = pd.DataFrame({'a':[11,21,31],'b':[12,22,312]})\n",
    "\n",
    "torch_tensor = torch.from_numpy(pd_tensor.values)\n",
    "\n",
    "numpy_tensor = torch_tensor.numpy()\n",
    "\n",
    "torch_tensor_again = torch.from_numpy(numpy_tensor)\n",
    "\n",
    "torch_tensor_again == torch_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cfedb1",
   "metadata": {},
   "source": [
    "Slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f0eb0528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the value on 2nd-row 3rd-column?  tensor(23)\n",
      "What is the value on 2nd-row 3rd-column?  tensor(23)\n",
      "What is the value on 1st-row first two columns?  tensor([11, 12])\n",
      "What is the value on 1st-row first two columns?  tensor([11, 12])\n",
      "What is the value on 3rd-column last two rows?  tensor([23, 33])\n"
     ]
    }
   ],
   "source": [
    "tensor_example = torch.tensor([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\n",
    "\n",
    "print(\"What is the value on 2nd-row 3rd-column? \", tensor_example[1, 2])\n",
    "print(\"What is the value on 2nd-row 3rd-column? \", tensor_example[1][2])\n",
    "\n",
    "print(\"What is the value on 1st-row first two columns? \", tensor_example[0, 0:2])\n",
    "print(\"What is the value on 1st-row first two columns? \", tensor_example[0][0:2])\n",
    "\n",
    "# Only thus is possible!!\n",
    "print(\"What is the value on 3rd-column last two rows? \", tensor_example[1:3, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7edb8e",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87929732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([[11., 12., 13.],\n",
    "                    [21., 22., 23.],\n",
    "                    [31., 32., 33.],\n",
    "                    [41., 42., 43.]])\n",
    "tensor.sum()\n",
    "tensor.mean() == tensor.sum()/tensor.numel() # True\n",
    "\n",
    "tensor.mean(axis = 0) # mean on the column! so this has size torch.Size([3])\n",
    "tensor.mean(axis = 1) # mean on the rows! so this has size torch.Size([4])\n",
    "\n",
    "np.mean(tensor.numpy()) == tensor.mean().item() # True\n",
    "\n",
    "np.mean(tensor.numpy(), axis = 0) == tensor.mean(axis = 0).numpy() # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83994798",
   "metadata": {},
   "source": [
    "### Operations among tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "63372307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2],\n",
       "        [0, 2]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[0, 1, 1], [1, 0, 1]])\n",
    "B = torch.tensor([[1, 1], [1, 1], [-1, 1]])\n",
    "\n",
    "torch.mm(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacccafb",
   "metadata": {},
   "source": [
    "## Across dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acdc901",
   "metadata": {},
   "source": [
    "| Function                     | Description                                                 | Shape Example                      |\n",
    "|------------------------------|-------------------------------------------------------------|------------------------------------|\n",
    "| `squeeze(x)`                 | Removes **all** dimensions of size 1                        | `(1, 3, 1, 5)` → `(3, 5)`          |\n",
    "| `squeeze(x, dim=0)`          | Removes dimension 0 **only if its size is 1**              | `(1, 3, 1, 5)` → `(3, 1, 5)`       |\n",
    "| `squeeze(x, dim=2)`          | Removes dimension 2 **only if its size is 1**              | `(1, 3, 1, 5)` → `(1, 3, 5)`       |\n",
    "| `unsqueeze(x, dim=0)`        | Adds a new dimension of size 1 at position 0               | `(28, 28)` → `(1, 28, 28)`         |\n",
    "| `unsqueeze(x, dim=2)`        | Adds a new dimension of size 1 at position 2               | `(3, 28, 28)` → `(3, 28, 1, 28)`   |\n",
    "| `unsqueeze(x, dim=-1)`       | Adds a new dimension of size 1 at the last position        | `(3, 28)` → `(3, 28, 1)`           |\n",
    "| `x.reshape(...)`      | Reshapes tensor to `new_shape` if total elements match     | `(1,3,1,5)` → `reshape(-1, 15)` → `(1,15)` |\n",
    "| | |`(1,3,1,5)` → `reshpae(-1)` → `(15)` |\n",
    "| `x.view(...)`         | Same as reshape but requires contiguous tensor             | `(1,3,1,5)` → `view(-1,15)` → `(1,15)`     |\n",
    "| | |`(1,3,1,5)` → `view(-1)` → `(15)` |\n",
    "| | equivalent to `x.flatten()`|| |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13bf4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 1, 5) # torch.Size([1, 3, 1, 5])\n",
    "\n",
    "torch.squeeze(x, dim = 2) == x.squeeze(dim = 2) #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e31be7",
   "metadata": {},
   "source": [
    "# Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69006511",
   "metadata": {},
   "source": [
    "**Torch tensor with required_grad = True to Numpy**: The method  `detach()` excludes further tracking of operations in the graph, and therefore the subgraph will not record operations. This allows us to then convert the tensor to a numpy array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a52a6d9",
   "metadata": {},
   "source": [
    "## Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a969aeab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad = True)\n",
    "y = x ** 3 # y.requires_grad --> True\n",
    "\n",
    "y.backward() # perform derivative\n",
    "\n",
    "x.grad # evaluate derivative at x --> tensor(12.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7f77bc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: tensor(8.)\n",
      "grad_fn: <PowBackward0 object at 0x16009b670>\n",
      "grad: None\n",
      "is_leaf: False\n",
      "requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "print('data:',y.data)\n",
    "print('grad_fn:',y.grad_fn)\n",
    "print('grad:',y.grad)\n",
    "print(\"is_leaf:\",y.is_leaf)\n",
    "print(\"requires_grad:\",y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5415c3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: tensor(2.)\n",
      "grad_fn: None\n",
      "grad: tensor(12.)\n",
      "is_leaf: True\n",
      "requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "print('data:',x.data)\n",
    "print('grad_fn:',x.grad_fn)\n",
    "print('grad:',x.grad)\n",
    "print(\"is_leaf:\",x.is_leaf)\n",
    "print(\"requires_grad:\",x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d956895",
   "metadata": {},
   "source": [
    "## Partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96124640",
   "metadata": {},
   "source": [
    "The following two methods are equivalent:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57623b3",
   "metadata": {},
   "source": [
    "1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "350c46b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 1.])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor(1.0,requires_grad=True)\n",
    "v = torch.tensor(2.0,requires_grad=True)\n",
    "f = u * v + u ** 2\n",
    "\n",
    "f.backward()\n",
    "\n",
    "u.grad # derivative of f wrt to u, computed in (u,v)\n",
    "v.grad\n",
    "\n",
    "torch.tensor([u.grad ,v.grad\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1135269",
   "metadata": {},
   "source": [
    "2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "cb4c67eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 1.])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "f = x[0] * x[1] + x[0] ** 2\n",
    "f.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdc156c",
   "metadata": {},
   "source": [
    "# From CSV to a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afab35e",
   "metadata": {},
   "source": [
    "### CSV load and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a5025c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-24 14:07:27--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/labs/Week1/data/img.tar.gz\n",
      "Risoluzione di cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.45.118.108\n",
      "Connessione a cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.45.118.108|:443... connesso.\n",
      "Richiesta HTTP inviata, in attesa di risposta... 200 OK\n",
      "Lunghezza: 50460 (49K) [application/x-tar]\n",
      "Salvataggio in: «img.tar.gz»\n",
      "\n",
      "img.tar.gz          100%[===================>]  49.28K   251KB/s    in 0.2s    \n",
      "\n",
      "2025-07-24 14:07:28 (251 KB/s) - «img.tar.gz» salvato [50460/50460]\n",
      "\n",
      "--2025-07-24 14:07:29--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/labs/Week1/data/index.csv\n",
      "Risoluzione di cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.45.118.108\n",
      "Connessione a cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.45.118.108|:443... connesso.\n",
      "Richiesta HTTP inviata, in attesa di risposta... 200 OK\n",
      "Lunghezza: 1680905 (1.6M) [text/csv]\n",
      "Salvataggio in: «index.csv»\n",
      "\n",
      "index.csv           100%[===================>]   1.60M  1.42MB/s    in 1.1s    \n",
      "\n",
      "2025-07-24 14:07:31 (1.42 MB/s) - «index.csv» salvato [1680905/1680905]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scarica il file nella directory corrente\n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/labs/Week1/data/img.tar.gz\n",
    "# Estrai il contenuto nella directory corrente\n",
    "!tar -xf img.tar.gz\n",
    "# Scarica anche il file index.csv\n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/labs/Week1/data/index.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "32f51ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ankle boot</td>\n",
       "      <td>img/fashion0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T-shirt</td>\n",
       "      <td>img/fashion1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T-shirt</td>\n",
       "      <td>img/fashion2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dress</td>\n",
       "      <td>img/fashion3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T-shirt</td>\n",
       "      <td>img/fashion4.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category             image\n",
       "0  Ankle boot  img/fashion0.png\n",
       "1     T-shirt  img/fashion1.png\n",
       "2     T-shirt  img/fashion2.png\n",
       "3       Dress  img/fashion3.png\n",
       "4     T-shirt  img/fashion4.png"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_name = pd.read_csv(\"index.csv\")\n",
    "data_name.shape #(60000,2)\n",
    "data_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b7d0dda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_name = data_name.iloc[1, 1]\n",
    "image_path = os.path.join(\"\",image_name)\n",
    "image = Image.open(image_path)\n",
    "plt.imshow(image,cmap='gray', vmin=0, vmax=255)\n",
    "plt.title(data_name.iloc[1, 0])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc762c44",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb112b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, csv_file, data_dir, transform = None):\n",
    "        \n",
    "        # Image directory\n",
    "        self.data_dir=data_dir\n",
    "        \n",
    "        # The transform is going to be used on image\n",
    "        self.transform = transform\n",
    "        data_dircsv_file = os.path.join(self.data_dir, csv_file)\n",
    "        # Load the CSV file contians image info\n",
    "        self.data_name = pd.read_csv(data_dircsv_file)\n",
    "        \n",
    "        # Shape of dataset\n",
    "        self.shape = self.data_name.shape \n",
    "        self.len = self.data_name.shape[0]\n",
    "    \n",
    "    # Get the length\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    # Get the shape\n",
    "    def __shape__(self):\n",
    "        return self.shape\n",
    "    \n",
    "    # Getter\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Image file path\n",
    "        img_name = os.path.join(self.data_dir,self.data_name.iloc[idx, 1])\n",
    "        # Open image file\n",
    "        image = Image.open(img_name)\n",
    "        \n",
    "        # The class label for the image\n",
    "        y = self.data_name.iloc[idx, 0]\n",
    "        \n",
    "        # If there is any transform method, apply it onto the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, y # this means that dataset[i][0] is the i-th image and dataset[i][1] is the i-th label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5a5ba273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fliptensor_data_transform = transforms.Compose([transforms.CenterCrop(35),\n",
    "                                                transforms.RandomVerticalFlip(p = 1),\n",
    "                                                transforms.ToTensor()]) # values in [0,1]\n",
    "\n",
    "dataset = Dataset(csv_file = \"index.csv\", data_dir = \"\", transform = fliptensor_data_transform)\n",
    "\n",
    "len(dataset) == dataset.len # 60000\n",
    "dataset.shape # (60000, 2)\n",
    "\n",
    "plt.imshow(dataset[0][0].squeeze(),cmap='gray', vmin=0, vmax=1)\n",
    "plt.title(f\"Given label: {dataset[0][1]}\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f50d727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:08<00:00, 1194953.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 259119.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 2165508.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 1552736.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.MNIST(\n",
    "    root = './mnist',  \n",
    "    download = True, \n",
    "    transform = transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36d01715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of the first element:  <class 'tuple'>\n",
      "The length of the tuple:  2\n",
      "The shape of the first element in the tuple:  torch.Size([1, 28, 28])\n",
      "The type of the first element in the tuple <class 'torch.Tensor'>\n",
      "The second element in the tuple:  5\n",
      "The type of the second element in the tuple:  <class 'int'>\n",
      "As the result, the structure of the first element in the dataset is (tensor([1, 28, 28]), tensor(7)).\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of the first element: \", type(dataset[0]))\n",
    "print(\"The length of the tuple: \", len(dataset[0]))\n",
    "print(\"The shape of the first element in the tuple: \", dataset[0][0].shape)\n",
    "print(\"The type of the first element in the tuple\", type(dataset[0][0]))\n",
    "print(\"The second element in the tuple: \", dataset[0][1])\n",
    "print(\"The type of the second element in the tuple: \", type(dataset[0][1]))\n",
    "print(\"As the result, the structure of the first element in the dataset is (tensor([1, 28, 28]), tensor(7)).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
